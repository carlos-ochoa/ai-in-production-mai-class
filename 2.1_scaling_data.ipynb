{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8cb167-9fcf-4958-92ed-336e281d924a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ============================================================================\n",
    "# EJEMPLO: Pandas vs Spark - Limitaciones y Escalabilidad\n",
    " ============================================================================\n",
    "### Pandas vs Spark: Cuando Pandas se queda corto\n",
    "\n",
    "Este notebook demuestra:\n",
    "1. Las limitaciones de memoria de Pandas\n",
    "2. C√≥mo Spark escala el mismo problema\n",
    "3. Diferencias en procesamiento y performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987bcecf-7d20-43bb-b83c-1e7ab3def0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16e7e82d-48be-49b0-adb8-91a63cd0dec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using Spark to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eba7930-e31b-47b7-82c5-bb4163920023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"GENERANDO DATASET DE EJEMPLO: 100 millones de registros de ventas\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Definir esquema para Spark\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", LongType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"region\", StringType(), False)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "932bf561-f5b2-4b8f-bc03-ff6010686cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_records = 100_000_000  # 100 millones de registros\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\n",
    "regions = [\"North\", \"South\", \"East\", \"West\", \"Central\"]\n",
    "\n",
    "print(f\"\\nCreando {num_records:,} registros con Spark...\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd0cead-a88d-4580-b3e3-f4a3a44e3ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark = (spark.range(num_records)\n",
    "    .withColumn(\"transaction_id\", F.col(\"id\"))\n",
    "    .withColumn(\"customer_id\", (F.rand() * 1_000_000).cast(\"int\"))\n",
    "    .withColumn(\"product_id\", (F.rand() * 10_000).cast(\"int\"))\n",
    "    .withColumn(\"category\", F.array(*[F.lit(c) for c in categories])[\n",
    "        (F.rand() * len(categories)).cast(\"int\")])\n",
    "    .withColumn(\"amount\", F.round(F.rand() * 1000 + 10, 2))\n",
    "    .withColumn(\"quantity\", (F.rand() * 10 + 1).cast(\"int\"))\n",
    "    .withColumn(\"timestamp\", F.expr(\"date_sub(current_timestamp(), cast(rand() * 365 as int))\"))\n",
    "    .withColumn(\"region\", F.array(*[F.lit(r) for r in regions])[\n",
    "        (F.rand() * len(regions)).cast(\"int\")])\n",
    "    .drop(\"id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e70a2d-5c40-420d-95ca-e747f65eb53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_spark.cache()\n",
    "count_spark = df_spark.count()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Dataset creado: {count_spark:,} registros en {elapsed:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d363dfe-b4a3-4228-b205-0fa49a9d3f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using pandas to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b662f166-ec44-4780-9254-cd22959442b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 1: INTENTANDO CARGAR TODO EN PANDAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Intentar cargar una porci√≥n grande en Pandas\n",
    "    sample_size = 10_000_000  # 10 millones (solo el 10% del dataset)\n",
    "    \n",
    "    print(f\"\\nIntentando cargar {sample_size:,} registros en Pandas...\")\n",
    "    print(\"‚ö†Ô∏è  ADVERTENCIA: Esto consumir√° mucha memoria RAM\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convertir muestra a Pandas\n",
    "    df_pandas = df_spark.limit(sample_size).toPandas()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    # deep true nos da el uso real de memoria al considerar objetos object\n",
    "    memory_mb = df_pandas.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    \n",
    "    print(f\"‚úì Cargado en {elapsed:.2f} segundos\")\n",
    "    print(f\"üìä Memoria utilizada: {memory_mb:,.2f} MB\")\n",
    "    print(f\"‚ö†Ô∏è  Esto es solo el {(sample_size/num_records)*100:.1f}% del dataset total\")\n",
    "    print(f\"‚ö†Ô∏è  Cargar el 100% requerir√≠a ~{(memory_mb * num_records / sample_size):,.0f} MB\")\n",
    "    print(\"\\nüí° PROBLEMA DE PANDAS:\")\n",
    "    print(\"   - Todo debe caber en la memoria RAM de un solo nodo\")\n",
    "    print(\"   - No hay paralelizaci√≥n distribuida\")\n",
    "    print(\"   - Escalar requiere hardware m√°s grande (scale-up)\")\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(\"‚ùå ERROR: MemoryError - No hay suficiente RAM!\")\n",
    "    print(f\"   {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7828201-43f5-4611-9edc-93d87f7cac37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " ============================================================================\n",
    "## OPERACIONES COMPLEJAS\n",
    " ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199ea4ba-228b-4012-91e9-006a549dc9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CASO 2: AGREGACIONES COMPLEJAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Operaci√≥n: An√°lisis de ventas por regi√≥n, categor√≠a y mes\n",
    "print(\"\\nüìä An√°lisis: Ventas totales y promedio por regi√≥n, categor√≠a y mes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b0433c-27ab-4481-98d3-422483cf7827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Con Pandas (en la muestra) ---\n",
    "if 'df_pandas' in locals():\n",
    "    print(\"\\n[PANDAS] Procesando 10M registros...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_pandas['year_month'] = pd.to_datetime(df_pandas['timestamp']).dt.to_period('M')\n",
    "    \n",
    "    result_pandas = (df_pandas\n",
    "        .groupby(['region', 'category', 'year_month'])\n",
    "        .agg({\n",
    "            'amount': ['sum', 'mean', 'count'],\n",
    "            'quantity': 'sum'\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    elapsed_pandas = time.time() - start_time\n",
    "    print(f\"‚úì Tiempo: {elapsed_pandas:.2f} segundos\")\n",
    "    print(f\"‚úì Registros resultantes: {len(result_pandas):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7529d186-0039-4e07-b289-fdb80e154b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Con Spark (dataset completo) ---\n",
    "print(\"\\n[SPARK] Procesando 100M registros (10x m√°s datos)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "result_spark = (df_spark\n",
    "    .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "    .groupBy(\"region\", \"category\", \"year_month\")\n",
    "    .agg(\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "        F.count(\"*\").alias(\"count_transactions\"),\n",
    "        F.sum(\"quantity\").alias(\"total_quantity\")\n",
    "    )\n",
    "    .orderBy(\"region\", \"category\", \"year_month\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04972ce5-e711-4a90-80b3-eb12dedea5cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_count = result_spark.count()\n",
    "elapsed_spark = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Tiempo: {elapsed_spark:.2f} segundos\")\n",
    "print(f\"‚úì Registros resultantes: {result_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf2bdeb-7cfb-43c5-8476-8eadc4b870cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# locals() es una funci√≥n de python que nos permite inspeccionar las variables locales creadas\n",
    "# hasta el momento, globals() nos da acceso a las variables globales\n",
    "\n",
    "print(\"\\nüìà COMPARACI√ìN:\")\n",
    "if 'elapsed_pandas' in locals():\n",
    "    print(f\"   Pandas:  {elapsed_pandas:.2f}s para 10M registros\")\n",
    "    print(f\"   Spark:   {elapsed_spark:.2f}s para 100M registros (10x m√°s datos)\")\n",
    "    speedup = (elapsed_pandas * 10) / elapsed_spark\n",
    "    print(f\"   üöÄ Spark es ~{speedup:.1f}x m√°s r√°pido en datos proporcionales\")\n",
    "\n",
    "# Mostrar muestra de resultados\n",
    "print(\"\\nüìã Muestra de resultados:\")\n",
    "result_spark.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117a6ce9-a85a-4d3d-aeaf-18e8d100c952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "====================================================\n",
    "## Comparando joins\n",
    "===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6782e1f0-bcd1-4b54-8d5c-692b6aa052de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"JOIN DE GRANDES DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nCreando tabla de clientes (5M registros)...\")\n",
    "df_customers = (spark.range(5_000_000)\n",
    "    .withColumn(\"customer_id\", F.col(\"id\").cast(\"int\"))\n",
    "    .withColumn(\"customer_name\", F.concat(F.lit(\"Customer_\"), F.col(\"id\")))\n",
    "    .withColumn(\"customer_segment\", \n",
    "        F.when(F.rand() < 0.3, \"Premium\")\n",
    "        .when(F.rand() < 0.6, \"Standard\")\n",
    "        .otherwise(\"Basic\"))\n",
    "    .withColumn(\"registration_date\", \n",
    "        F.expr(\"date_sub(current_timestamp(), cast(rand() * 1000 as int))\"))\n",
    "    .drop(\"id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49f686b-5ac3-4c66-b22e-0da3d9273a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n[SPARK] Join de 100M transacciones con 5M clientes...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_enriched = (df_spark\n",
    "    .join(df_customers, \"customer_id\", \"left\")\n",
    "    .select(\n",
    "        \"transaction_id\",\n",
    "        \"customer_id\",\n",
    "        \"customer_name\",\n",
    "        \"customer_segment\",\n",
    "        \"category\",\n",
    "        \"amount\",\n",
    "        \"region\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e2d894-ce54-4877-bfeb-18eb82483d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcular m√©tricas por segmento de cliente\n",
    "segment_analysis = (df_enriched\n",
    "    .groupBy(\"customer_segment\", \"category\")\n",
    "    .agg(\n",
    "        F.sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.avg(\"amount\").alias(\"avg_transaction\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_sales\"))\n",
    ")\n",
    "\n",
    "result_count = segment_analysis.count()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"‚úì Join completado en {elapsed:.2f} segundos\")\n",
    "print(f\"‚úì Registros analizados: {result_count:,}\")\n",
    "\n",
    "print(\"\\nüìã An√°lisis por segmento de cliente:\")\n",
    "segment_analysis.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6005d9-c41f-43e9-8059-cc8ce498dc41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nüí° POR QU√â SPARK ESCALA:\")\n",
    "print(\"   ‚úì Procesamiento distribuido en m√∫ltiples nodos\")\n",
    "print(\"   ‚úì Lazy evaluation: optimiza el plan de ejecuci√≥n\")\n",
    "print(\"   ‚úì Datos particionados: trabaja con chunks en paralelo\")\n",
    "print(\"   ‚úì Shuffle optimization para joins eficientes\")\n",
    "print(\"   ‚úì Spill to disk: usa disco si se queda sin RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03e7979-0e43-46ba-bd76-f22136d8bed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- INTENTO CON PANDAS (muestra peque√±a) ---\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"INTENTO CON PANDAS (muestra reducida)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Usamos muestras mucho m√°s peque√±as para Pandas\n",
    "pandas_sample_transactions = 500_000  # Solo 500K transacciones\n",
    "pandas_sample_customers = 100_000     # Solo 100K clientes\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Usando muestra reducida para Pandas:\")\n",
    "print(f\"   - Transacciones: {pandas_sample_transactions:,} (0.5% del total)\")\n",
    "print(f\"   - Clientes: {pandas_sample_customers:,} (2% del total)\")\n",
    "print(f\"   - Esto es necesario por limitaciones de memoria de Pandas\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n[PANDAS] Convirtiendo datos a Pandas...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convertir muestras a Pandas\n",
    "    df_transactions_pandas = df_spark.limit(pandas_sample_transactions).toPandas()\n",
    "    df_customers_pandas = df_customers.limit(pandas_sample_customers).toPandas()\n",
    "    \n",
    "    conversion_time = time.time() - start_time\n",
    "    print(f\"‚úì Conversi√≥n completada en {conversion_time:.2f} segundos\")\n",
    "    \n",
    "    # Calcular memoria usada\n",
    "    memory_transactions = df_transactions_pandas.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    memory_customers = df_customers_pandas.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    total_memory = memory_transactions + memory_customers\n",
    "    \n",
    "    print(f\"üìä Memoria utilizada:\")\n",
    "    print(f\"   - Transacciones: {memory_transactions:,.2f} MB\")\n",
    "    print(f\"   - Clientes: {memory_customers:,.2f} MB\")\n",
    "    print(f\"   - Total: {total_memory:,.2f} MB\")\n",
    "    \n",
    "    # Proyecci√≥n de memoria para dataset completo\n",
    "    projected_memory = (memory_transactions * (num_records / pandas_sample_transactions)) + \\\n",
    "                       (memory_customers * (5_000_000 / pandas_sample_customers))\n",
    "    print(f\"‚ö†Ô∏è  Memoria proyectada para dataset completo: ~{projected_memory:,.0f} MB ({projected_memory/1024:.1f} GB)\")\n",
    "    \n",
    "    print(\"\\n[PANDAS] Ejecutando JOIN...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Realizar el join en Pandas\n",
    "    df_enriched_pandas = df_transactions_pandas.merge(\n",
    "        df_customers_pandas,\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calcular m√©tricas por segmento\n",
    "    segment_analysis_pandas = (df_enriched_pandas\n",
    "        .groupby(['customer_segment', 'category'])\n",
    "        .agg({\n",
    "            'amount': ['sum', 'mean'],\n",
    "            'customer_id': 'nunique'\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Renombrar columnas para consistencia\n",
    "    segment_analysis_pandas.columns = [\n",
    "        'customer_segment', 'category', \n",
    "        'total_sales', 'avg_transaction', 'unique_customers'\n",
    "    ]\n",
    "    \n",
    "    # Ordenar por ventas totales\n",
    "    segment_analysis_pandas = segment_analysis_pandas.sort_values(\n",
    "        'total_sales', ascending=False\n",
    "    )\n",
    "    \n",
    "    elapsed_pandas = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úì JOIN completado en {elapsed_pandas:.2f} segundos\")\n",
    "    print(f\"‚úì Registros despu√©s del join: {len(df_enriched_pandas):,}\")\n",
    "    print(f\"‚úì Grupos analizados: {len(segment_analysis_pandas):,}\")\n",
    "    \n",
    "    print(\"\\nüìã Primeros resultados (Pandas):\")\n",
    "    print(segment_analysis_pandas.head(10).to_string(index=False))\n",
    "    \n",
    "    # Proyectar tiempo para dataset completo\n",
    "    projected_time = elapsed_pandas * (num_records / pandas_sample_transactions)\n",
    "    print(f\"\\n‚è±Ô∏è  Tiempo proyectado para dataset completo: ~{projected_time:.0f} segundos ({projected_time/60:.1f} minutos)\")\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(\"‚ùå ERROR: MemoryError durante el JOIN!\")\n",
    "    print(f\"   Pandas no puede manejar este volumen de datos\")\n",
    "    print(f\"   Incluso la muestra reducida es demasiado grande\")\n",
    "    elapsed_pandas = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: {str(e)}\")\n",
    "    elapsed_pandas = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9947804e-226c-4de3-80a6-d9a69b14ac2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "RESUMEN: CU√ÅNDO USAR CADA HERRAMIENTA\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   CRITERIO      ‚îÇ         PANDAS           ‚îÇ          SPARK           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ Tama√±o datos    ‚îÇ < 10 GB                  ‚îÇ > 10 GB a Petabytes      ‚îÇ\n",
    "‚îÇ Memoria         ‚îÇ Todo debe caber en RAM   ‚îÇ Procesa por particiones  ‚îÇ\n",
    "‚îÇ Escalabilidad   ‚îÇ Vertical (scale-up)      ‚îÇ Horizontal (scale-out)   ‚îÇ\n",
    "‚îÇ Velocidad       ‚îÇ R√°pido en datos peque√±os ‚îÇ Optimizado para Big Data ‚îÇ\n",
    "‚îÇ Facilidad uso   ‚îÇ API simple e intuitiva   ‚îÇ Curva de aprendizaje     ‚îÇ\n",
    "‚îÇ Iteraci√≥n       ‚îÇ Interactivo, inmediato   ‚îÇ Lazy evaluation          ‚îÇ\n",
    "‚îÇ Mejor para      ‚îÇ An√°lisis exploratorio    ‚îÇ ETL y producci√≥n         ‚îÇ\n",
    "‚îÇ                 ‚îÇ Prototipado r√°pido       ‚îÇ Pipelines distribuidos   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "üéØ LECCIONES CLAVE:\n",
    "\n",
    "1. L√çMITES DE MEMORIA\n",
    "   - Pandas: Si tus datos no caben en RAM, est√°s en problemas\n",
    "   - Spark: Divide y conquista con procesamiento distribuido\n",
    "\n",
    "2. ESCALABILIDAD\n",
    "   - Pandas: Agregar RAM es caro y tiene l√≠mites f√≠sicos\n",
    "   - Spark: Agregar nodos es lineal y pr√°cticamente ilimitado\n",
    "\n",
    "3. PERFORMANCE\n",
    "   - Pandas: Excelente para < 1GB, single-threaded principalmente\n",
    "   - Spark: Dise√±ado para TB/PB con paralelizaci√≥n autom√°tica\n",
    "\n",
    "4. CU√ÅNDO USAR QU√â\n",
    "   - Usa Pandas: Datasets < 10GB, an√°lisis r√°pido, notebooks\n",
    "   - Usa Spark: Datasets > 10GB, ETL, producci√≥n, m√∫ltiples fuentes\n",
    "\n",
    "5. ESTRATEGIA H√çBRIDA\n",
    "   - Procesa con Spark, analiza muestras con Pandas\n",
    "   - Usa Spark SQL para filtrar, luego .toPandas() en resultados\n",
    "\n",
    "‚úÖ Notebook completado. Compara los tiempos y memoria utilizados.\n",
    "üí° Experimenta cambiando num_records para ver el impacto en Pandas vs Spark\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72b2444-b1c9-4b9f-9819-763d43836883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.1_scaling_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
