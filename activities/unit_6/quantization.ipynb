{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b38e91-9211-44c6-8723-4193e406c06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch torchao accelerate bitsandbytes\n",
    "# We could use uv in other enviroments as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "058a8654-847e-4844-a589-2870eb6c9a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad3dd55-5e92-4071-9d98-d2a7a031c8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quantization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad0aa10-2088-451c-ba98-4a64fd2b551a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f2ee54-dd19-4ce1-bf69-a9f355abe200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ============================================\n",
    "### 1. CARGAR MODELO NORMAL (FP32/FP16)\n",
    "### ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd71aff-1228-4a27-a1e5-ff949925d889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Podemos cargar con Transformers de HF apoyándonos de su librería bitsandbytes\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Cargar modelo en FP32\n",
    "model_fp32 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# Cargar modelo en INT8 (automático con bitsandbytes)\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Medir tamaños\n",
    "def get_model_size_mb(model):\n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        total_bytes += param.nelement() * param.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        total_bytes += buffer.nelement() * buffer.element_size()\n",
    "    return total_bytes / (1024 * 1024)\n",
    "\n",
    "size_fp32 = get_model_size_mb(model_fp32)\n",
    "size_int8 = get_model_size_mb(model_int8)\n",
    "\n",
    "print(f\"\\n=== COMPARACIÓN GPT-2 ===\")\n",
    "print(f\"FP32: {size_fp32:.2f} MB\")\n",
    "print(f\"INT8: {size_int8:.2f} MB\")\n",
    "print(f\"Reducción: {(1 - size_int8/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c82baf2-d868-4e5a-857c-b79163ce2d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "# Cargar modelo FP32\n",
    "model_fp32 = AutoModel.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    torch_dtype=torch.float32\n",
    ").cpu()\n",
    "\n",
    "print(f\"Modelo cargado: DistilBERT\")\n",
    "\n",
    "# Cuantizar\n",
    "model_int8 = copy.deepcopy(model_fp32)\n",
    "model_int8.eval()\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_int8,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Medir\n",
    "def get_size(model):\n",
    "    total = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
    "    total += sum(b.nelement() * b.element_size() for b in model.buffers())\n",
    "    return total / (1024 * 1024)\n",
    "\n",
    "size_fp32 = get_size(model_fp32)\n",
    "size_int8 = get_size(model_int8)\n",
    "\n",
    "print(f\"\\n=== COMPARACIÓN ===\")\n",
    "print(f\"FP32: {size_fp32:.2f} MB\")\n",
    "print(f\"INT8: {size_int8:.2f} MB\")\n",
    "print(f\"Reducción: {(1 - size_int8/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a71844-7617-4943-af91-b0f3d1946443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Comparemos el trade-off ahora en velocidad de inferencia y precisión del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b96eb7d6-97ff-41c8-926b-2213066f10e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Crear input de prueba\n",
    "inputs = torch.randint(0, 30522, (1, 128))  # batch=1, seq_len=128\n",
    "\n",
    "# Benchmark FP32\n",
    "model_fp32.eval()\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model_fp32(inputs)\n",
    "    time_fp32 = (time.time() - start) / 100\n",
    "\n",
    "# Benchmark INT8\n",
    "model_int8.eval()\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = model_int8(inputs)\n",
    "    time_int8 = (time.time() - start) / 100\n",
    "\n",
    "print(f\"\\n=== VELOCIDAD DE INFERENCIA ===\")\n",
    "print(f\"FP32: {time_fp32*1000:.2f} ms\")\n",
    "print(f\"INT8: {time_int8*1000:.2f} ms\")\n",
    "print(f\"Speedup: {time_fp32/time_int8:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96656530-f0f9-4a53-9a8d-14c8e361298c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Texto de prueba\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Comparar outputs\n",
    "with torch.no_grad():\n",
    "    out_fp32 = model_fp32(**inputs).last_hidden_state\n",
    "    out_int8 = model_int8(**inputs).last_hidden_state\n",
    "\n",
    "# Calcular diferencias\n",
    "diff = (out_fp32 - out_int8).abs()\n",
    "\n",
    "print(f\"\\n=== PÉRDIDA DE PRECISIÓN ===\")\n",
    "print(f\"Diferencia promedio: {diff.mean().item():.6f}\")\n",
    "print(f\"Diferencia máxima:   {diff.max().item():.6f}\")\n",
    "print(f\"Diferencia relativa: {(diff.mean() / out_fp32.abs().mean()).item()*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "quantization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
