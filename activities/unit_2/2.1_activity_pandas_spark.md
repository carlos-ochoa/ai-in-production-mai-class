# ğŸ¯ Actividad PrÃ¡ctica: MigraciÃ³n Progresiva de Pandas a Spark

## Objetivo de Aprendizaje
Aprender a identificar cuÃ¡ndo un anÃ¡lisis en Pandas necesita migrar a Spark y ejecutar la migraciÃ³n de forma incremental.

---

## ğŸ“‹ Contexto del Problema

Eres analista de datos en **"E-Commerce Global"**, una empresa de comercio electrÃ³nico que estÃ¡ creciendo rÃ¡pidamente. Tu jefe te ha pedido un anÃ¡lisis de comportamiento de usuarios, pero los datos estÃ¡n creciendo exponencialmente.

**Dataset inicial:**

Para obtener los datasets primero ejecuta el script provisto en esta carpeta: generate_datasets.py

- `user_activity.csv`: 5 millones de registros de actividad de usuarios
- `products.csv`: 50,000 productos
- `orders.csv`: 2 millones de Ã³rdenes

**Tu tarea:** El anÃ¡lisis funciona bien con Pandas, pero los datos crecerÃ¡n a **100 millones de registros** el prÃ³ximo mes. Necesitas preparar el cÃ³digo para escalar.

---

## ğŸš€ FASE 1: AnÃ¡lisis Inicial con Pandas (30 minutos)

### Paso 1.1: Carga y ExploraciÃ³n de Datos

**Tu tarea:**
1. Carga los tres archivos CSV con Pandas
2. Realiza un anÃ¡lisis exploratorio bÃ¡sico:
   - Cantidad de registros en cada tabla
   - Tipos de datos
   - Valores nulos
   - EstadÃ­sticas descriptivas

**CÃ³digo base:**
```python
import pandas as pd
import time

# TODO: Cargar los datasets
df_activity = pd.read_csv('user_activity.csv')
df_products = pd.read_csv('products.csv')
df_orders = pd.read_csv('orders.csv')

# TODO: Explorar los datos
# - Â¿CuÃ¡ntos registros hay en cada tabla?
# - Â¿QuÃ© columnas tienen valores nulos?
# - Â¿CuÃ¡l es el rango de fechas en el dataset?
```

**Preguntas de reflexiÃ³n:**
- Â¿CuÃ¡nta memoria RAM estÃ¡ usando tu notebook?
- Â¿QuÃ© pasarÃ­a si los datos crecen 20x?

---

### Paso 1.2: AnÃ¡lisis de Negocio con Pandas

**Requerimientos del negocio:**

Calcula las siguientes mÃ©tricas:

1. **Top 10 productos mÃ¡s vendidos** por categorÃ­a
2. **Usuarios mÃ¡s activos**: usuarios con mÃ¡s de 50 actividades
3. **AnÃ¡lisis de conversiÃ³n**: 
   - Porcentaje de usuarios que vieron un producto y lo compraron
   - Tiempo promedio entre primera vista y compra
4. **AnÃ¡lisis temporal**:
   - Ventas por mes
   - Hora del dÃ­a con mÃ¡s actividad

**CÃ³digo para completar:**
```python
# TODO 1: Top productos por categorÃ­a
start = time.time()

# Join entre orders y products
orders_enriched = df_orders.merge(df_products, on='product_id', how='left')

# Agrupar y calcular
top_products = (orders_enriched
    .groupby(['category', 'product_name'])
    .agg({'order_id': 'count', 'total_amount': 'sum'})
    .reset_index()
    .sort_values(['category', 'total_amount'], ascending=[True, False])
)

print(f"Tiempo: {time.time() - start:.2f}s")

# TODO 2: Usuarios mÃ¡s activos
# Pista: usa groupby en df_activity

# TODO 3: AnÃ¡lisis de conversiÃ³n
# Pista: necesitas hacer join entre activity y orders

# TODO 4: AnÃ¡lisis temporal
# Pista: convierte timestamps a datetime y extrae componentes
```

**Entregable Fase 1:**
- Screenshot del uso de memoria (usa `df.memory_usage()`)
- Tiempos de ejecuciÃ³n de cada anÃ¡lisis
- Documento con tus respuestas a las mÃ©tricas de negocio

---

## âš ï¸ FASE 2: Identificando Cuellos de Botella (20 minutos)

### Paso 2.1: MediciÃ³n de Performance

**Tu tarea:** Completa la siguiente tabla midiendo el rendimiento:

| OperaciÃ³n | Tiempo (seg) | Memoria (MB) | Â¿Funciona con 100M registros? |
|-----------|--------------|--------------|-------------------------------|
| Carga de datos | | | |
| Join activity + orders | | | |
| GroupBy por categorÃ­a | | | |
| CÃ¡lculo de conversiÃ³n | | | |

**CÃ³digo para medir:**
```python
import psutil
import os

def measure_performance(func, *args):
    """Mide tiempo y memoria de una funciÃ³n"""
    process = psutil.Process(os.getpid())
    
    # Memoria antes
    mem_before = process.memory_info().rss / 1024 / 1024
    
    # Ejecutar
    start = time.time()
    result = func(*args)
    elapsed = time.time() - start
    
    # Memoria despuÃ©s
    mem_after = process.memory_info().rss / 1024 / 1024
    mem_used = mem_after - mem_before
    
    return result, elapsed, mem_used

# TODO: Usa esta funciÃ³n para medir cada operaciÃ³n
```

### Paso 2.2: AnÃ¡lisis de Escalabilidad

**Preguntas crÃ­ticas:**

1. **Â¿QuÃ© operaciÃ³n es la mÃ¡s costosa?** (tiempo y memoria)
2. **Â¿CuÃ¡l serÃ­a el consumo de memoria con 100M registros?** (proyecta proporcionalmente)
3. **Â¿QuÃ© operaciones causarÃ­an un MemoryError?**
4. **Si solo puedes migrar 2 operaciones a Spark, Â¿cuÃ¡les serÃ­an y por quÃ©?**

**Entregable Fase 2:**
- Tabla completa con mÃ©tricas
- Respuestas a las 4 preguntas crÃ­ticas
- Diagrama identificando los 3 cuellos de botella principales

---

## ğŸ”„ FASE 3: MigraciÃ³n EstratÃ©gica a Spark (45 minutos)

### Paso 3.1: MigraciÃ³n del JOIN (Alta Prioridad)

**Estrategia:** Migra solo la operaciÃ³n de JOIN a Spark, mantÃ©n lo demÃ¡s en Pandas.

**Tu tarea:**
```python
from pyspark.sql import functions as F

# 1. Carga los datos en Spark
df_activity_spark = spark.read.csv('user_activity.csv', header=True, inferSchema=True)
df_orders_spark = spark.read.csv('orders.csv', header=True, inferSchema=True)
df_products_spark = spark.read.csv('products.csv', header=True, inferSchema=True)

# 2. Realiza el JOIN en Spark
start = time.time()

# TODO: Implementa el join en Spark
orders_enriched_spark = (df_orders_spark
    .join(df_products_spark, on='product_id', how='left')
)

# 3. Convierte el RESULTADO a Pandas para anÃ¡lisis
orders_enriched_pandas = orders_enriched_spark.toPandas()

print(f"Tiempo JOIN con Spark: {time.time() - start:.2f}s")

# 4. ContinÃºa el anÃ¡lisis en Pandas
top_products = (orders_enriched_pandas
    .groupby(['category', 'product_name'])
    .agg({'order_id': 'count', 'total_amount': 'sum'})
    .reset_index()
)
```

**Compara:**
- Tiempo del JOIN: Pandas vs Spark
- Â¿CuÃ¡l es mÃ¡s rÃ¡pido? Â¿Por quÃ©?

---

### Paso 3.2: MigraciÃ³n de Agregaciones (Media Prioridad)

**Tu tarea:** Migra los GroupBy complejos a Spark

```python
# TODO: Calcula top productos usando SOLO Spark
top_products_spark = (orders_enriched_spark
    .groupBy('category', 'product_name')
    # Completa con las agregaciones necesarias
    .agg(
        # TODO: aÃ±ade agregaciones
    )
    .orderBy(# TODO: ordena apropiadamente)
)

# TODO: Compara el resultado con la versiÃ³n Pandas
# Â¿Son idÃ©nticos los resultados?
```

---

### Paso 3.3: Pipeline Completo en Spark (Avanzado)

**DesafÃ­o final:** Migra TODO el anÃ¡lisis a Spark

**Requisitos:**
1. Carga de datos â†’ Spark
2. Joins â†’ Spark
3. Agregaciones â†’ Spark
4. Solo convierte a Pandas el resultado final (para visualizaciÃ³n)

```python
# TODO: Implementa el anÃ¡lisis completo en Spark
def analisis_completo_spark():
    """
    Implementa las 4 mÃ©tricas de negocio usando solo Spark:
    1. Top productos por categorÃ­a
    2. Usuarios mÃ¡s activos
    3. AnÃ¡lisis de conversiÃ³n
    4. AnÃ¡lisis temporal
    
    Retorna: diccionario con los 4 resultados
    """
    pass

# Ejecuta y mide
start = time.time()
resultados = analisis_completo_spark()
tiempo_total = time.time() - start

print(f"AnÃ¡lisis completo en {tiempo_total:.2f}s")
```

**Entregable Fase 3:**
- CÃ³digo completo de las 3 migraciones
- Tabla comparativa de tiempos:
  - Pandas puro
  - HÃ­brido (Join en Spark)
  - Spark puro
- Â¿QuÃ© enfoque es mÃ¡s rÃ¡pido y por quÃ©?

---

## ğŸ“Š FASE 4: Estrategia de MigraciÃ³n (30 minutos)

### Paso 4.1: Matriz de DecisiÃ³n

Completa esta matriz para cada operaciÃ³n en tu cÃ³digo:

| OperaciÃ³n | TamaÃ±o datos | Complejidad | Â¿Frecuencia? | DecisiÃ³n | JustificaciÃ³n |
|-----------|--------------|-------------|--------------|----------|---------------|
| Carga CSV | 5M â†’ 100M | Baja | 1x/dÃ­a | Spark | Volumen alto |
| Filtros simples | | | | | |
| Join 2 tablas | | | | | |
| GroupBy 3 niveles | | | | | |
| VisualizaciÃ³n | | | | | |

**Criterios de decisiÃ³n:**
- âœ… **Migrar a Spark si:**
  - Datos > 10GB
  - Joins de tablas grandes
  - GroupBy con mÃºltiples niveles
  - Operaciones que se ejecutan frecuentemente
  
- âŒ **Mantener en Pandas si:**
  - Datos < 1GB
  - AnÃ¡lisis exploratorio rÃ¡pido
  - Resultado final para visualizaciÃ³n
  - Operaciones de una sola vez

---

### Paso 4.2: Plan de MigraciÃ³n

Crea un plan de migraciÃ³n para producciÃ³n:

```markdown
## Plan de MigraciÃ³n a Spark

### Fase 1: Quick Wins (Semana 1)
- [ ] Migrar operaciÃ³n: ___________
- [ ] Beneficio esperado: ___________
- [ ] Riesgo: ___________

### Fase 2: Operaciones Core (Semana 2-3)
- [ ] Migrar operaciÃ³n: ___________
- [ ] Beneficio esperado: ___________

### Fase 3: OptimizaciÃ³n (Semana 4)
- [ ] Tuning de particiones
- [ ] Caching estratÃ©gico
- [ ] Monitoreo de performance

### Criterios de Ã‰xito
- ReducciÃ³n de tiempo de procesamiento: ____%
- Capacidad de escalar a: _____M registros
- Costo de infraestructura: +/- ____%
```

---

## ğŸ“ Entregables Finales

### 1. Reporte TÃ©cnico (2-3 pÃ¡ginas)
Incluye:
- Resumen ejecutivo
- AnÃ¡lisis de cuellos de botella identificados
- Comparativa de performance (tablas y grÃ¡ficas)
- DecisiÃ³n de quÃ© migrar y quÃ© no
- CÃ³digo de las migraciones implementadas

### 2. PresentaciÃ³n (10 minutos)
Prepara slides para presentar:
- El problema de escalabilidad
- Estrategia de migraciÃ³n
- Resultados obtenidos
- Lecciones aprendidas

### 3. CÃ³digo Ejecutable
- Notebook con las 3 versiones (Pandas, HÃ­brido, Spark)
- Comentado y documentado
- Incluye mediciones de performance

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n
- [Pandas to PySpark Cheatsheet](https://spark.apache.org/docs/latest/api/python/)
- [OptimizaciÃ³n de Spark](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

### Criterios de EvaluaciÃ³n
- **IdentificaciÃ³n correcta de cuellos de botella (30%)**
- **ImplementaciÃ³n de migraciones (40%)**
- **JustificaciÃ³n de decisiones (20%)**
- **Calidad del cÃ³digo y documentaciÃ³n (10%)**

---

## ğŸ’¡ Tips para el Ã‰xito

1. **No migres todo de golpe** - Empieza con las operaciones mÃ¡s costosas
2. **Mide siempre** - No asumas, comprueba con datos
3. **MantÃ©n flexibilidad** - A veces un enfoque hÃ­brido es mejor
4. **Documenta decisiones** - Explica el "por quÃ©" de cada migraciÃ³n
5. **Piensa en el futuro** - Â¿Tu soluciÃ³n escalarÃ¡ a 1B de registros?

---

## ğŸ”¥ DesafÃ­o Extra (Opcional)

**Escenario:** Los datos ahora llegan en streaming (100K registros/minuto)

**Tu tarea:**
- Investiga Spark Structured Streaming
- PropÃ³n cÃ³mo adaptarÃ­as tu cÃ³digo
- Â¿QuÃ© operaciones funcionarÃ­an en streaming?
- Â¿CuÃ¡les necesitan batch processing?

---

## âœ… Checklist de Completitud

- [ ] Fase 1: AnÃ¡lisis con Pandas completado
- [ ] Fase 2: Cuellos de botella identificados
- [ ] Fase 3: Al menos 2 migraciones implementadas
- [ ] Fase 4: Estrategia documentada
- [ ] Reporte tÃ©cnico entregado
- [ ] CÃ³digo comentado y funcional
- [ ] PresentaciÃ³n preparada

**Â¡Buena suerte! ğŸš€**