# 🎯 Actividad Práctica: Migración Progresiva de Pandas a Spark

## Objetivo de Aprendizaje
Aprender a identificar cuándo un análisis en Pandas necesita migrar a Spark y ejecutar la migración de forma incremental.

---

## 📋 Contexto del Problema

Eres analista de datos en **"E-Commerce Global"**, una empresa de comercio electrónico que está creciendo rápidamente. Tu jefe te ha pedido un análisis de comportamiento de usuarios, pero los datos están creciendo exponencialmente.

**Dataset inicial:**

Para obtener los datasets primero ejecuta el script provisto en esta carpeta: generate_datasets.py

- `user_activity.csv`: 5 millones de registros de actividad de usuarios
- `products.csv`: 50,000 productos
- `orders.csv`: 2 millones de órdenes

**Tu tarea:** El análisis funciona bien con Pandas, pero los datos crecerán a **100 millones de registros** el próximo mes. Necesitas preparar el código para escalar.

---

## 🚀 FASE 1: Análisis Inicial con Pandas (30 minutos)

### Paso 1.1: Carga y Exploración de Datos

**Tu tarea:**
1. Carga los tres archivos CSV con Pandas
2. Realiza un análisis exploratorio básico:
   - Cantidad de registros en cada tabla
   - Tipos de datos
   - Valores nulos
   - Estadísticas descriptivas

**Código base:**
```python
import pandas as pd
import time

# TODO: Cargar los datasets
df_activity = pd.read_csv('user_activity.csv')
df_products = pd.read_csv('products.csv')
df_orders = pd.read_csv('orders.csv')

# TODO: Explorar los datos
# - ¿Cuántos registros hay en cada tabla?
# - ¿Qué columnas tienen valores nulos?
# - ¿Cuál es el rango de fechas en el dataset?
```

**Preguntas de reflexión:**
- ¿Cuánta memoria RAM está usando tu notebook?
- ¿Qué pasaría si los datos crecen 20x?

---

### Paso 1.2: Análisis de Negocio con Pandas

**Requerimientos del negocio:**

Calcula las siguientes métricas:

1. **Top 10 productos más vendidos** por categoría
2. **Usuarios más activos**: usuarios con más de 50 actividades
3. **Análisis de conversión**: 
   - Porcentaje de usuarios que vieron un producto y lo compraron
   - Tiempo promedio entre primera vista y compra
4. **Análisis temporal**:
   - Ventas por mes
   - Hora del día con más actividad

**Código para completar:**
```python
# TODO 1: Top productos por categoría
start = time.time()

# Join entre orders y products
orders_enriched = df_orders.merge(df_products, on='product_id', how='left')

# Agrupar y calcular
top_products = (orders_enriched
    .groupby(['category', 'product_name'])
    .agg({'order_id': 'count', 'total_amount': 'sum'})
    .reset_index()
    .sort_values(['category', 'total_amount'], ascending=[True, False])
)

print(f"Tiempo: {time.time() - start:.2f}s")

# TODO 2: Usuarios más activos
# Pista: usa groupby en df_activity

# TODO 3: Análisis de conversión
# Pista: necesitas hacer join entre activity y orders

# TODO 4: Análisis temporal
# Pista: convierte timestamps a datetime y extrae componentes
```

**Entregable Fase 1:**
- Screenshot del uso de memoria (usa `df.memory_usage()`)
- Tiempos de ejecución de cada análisis
- Documento con tus respuestas a las métricas de negocio

---

## ⚠️ FASE 2: Identificando Cuellos de Botella (20 minutos)

### Paso 2.1: Medición de Performance

**Tu tarea:** Completa la siguiente tabla midiendo el rendimiento:

| Operación | Tiempo (seg) | Memoria (MB) | ¿Funciona con 100M registros? |
|-----------|--------------|--------------|-------------------------------|
| Carga de datos | | | |
| Join activity + orders | | | |
| GroupBy por categoría | | | |
| Cálculo de conversión | | | |

**Código para medir:**
```python
import psutil
import os

def measure_performance(func, *args):
    """Mide tiempo y memoria de una función"""
    process = psutil.Process(os.getpid())
    
    # Memoria antes
    mem_before = process.memory_info().rss / 1024 / 1024
    
    # Ejecutar
    start = time.time()
    result = func(*args)
    elapsed = time.time() - start
    
    # Memoria después
    mem_after = process.memory_info().rss / 1024 / 1024
    mem_used = mem_after - mem_before
    
    return result, elapsed, mem_used

# TODO: Usa esta función para medir cada operación
```

### Paso 2.2: Análisis de Escalabilidad

**Preguntas críticas:**

1. **¿Qué operación es la más costosa?** (tiempo y memoria)
2. **¿Cuál sería el consumo de memoria con 100M registros?** (proyecta proporcionalmente)
3. **¿Qué operaciones causarían un MemoryError?**
4. **Si solo puedes migrar 2 operaciones a Spark, ¿cuáles serían y por qué?**

**Entregable Fase 2:**
- Tabla completa con métricas
- Respuestas a las 4 preguntas críticas
- Diagrama identificando los 3 cuellos de botella principales

---

## 🔄 FASE 3: Migración Estratégica a Spark (45 minutos)

### Paso 3.1: Migración del JOIN (Alta Prioridad)

**Estrategia:** Migra solo la operación de JOIN a Spark, mantén lo demás en Pandas.

**Tu tarea:**
```python
from pyspark.sql import functions as F

# 1. Carga los datos en Spark
df_activity_spark = spark.read.csv('user_activity.csv', header=True, inferSchema=True)
df_orders_spark = spark.read.csv('orders.csv', header=True, inferSchema=True)
df_products_spark = spark.read.csv('products.csv', header=True, inferSchema=True)

# 2. Realiza el JOIN en Spark
start = time.time()

# TODO: Implementa el join en Spark
orders_enriched_spark = (df_orders_spark
    .join(df_products_spark, on='product_id', how='left')
)

# 3. Convierte el RESULTADO a Pandas para análisis
orders_enriched_pandas = orders_enriched_spark.toPandas()

print(f"Tiempo JOIN con Spark: {time.time() - start:.2f}s")

# 4. Continúa el análisis en Pandas
top_products = (orders_enriched_pandas
    .groupby(['category', 'product_name'])
    .agg({'order_id': 'count', 'total_amount': 'sum'})
    .reset_index()
)
```

**Compara:**
- Tiempo del JOIN: Pandas vs Spark
- ¿Cuál es más rápido? ¿Por qué?

---

### Paso 3.2: Migración de Agregaciones (Media Prioridad)

**Tu tarea:** Migra los GroupBy complejos a Spark

```python
# TODO: Calcula top productos usando SOLO Spark
top_products_spark = (orders_enriched_spark
    .groupBy('category', 'product_name')
    # Completa con las agregaciones necesarias
    .agg(
        # TODO: añade agregaciones
    )
    .orderBy(# TODO: ordena apropiadamente)
)

# TODO: Compara el resultado con la versión Pandas
# ¿Son idénticos los resultados?
```

---

### Paso 3.3: Pipeline Completo en Spark (Avanzado)

**Desafío final:** Migra TODO el análisis a Spark

**Requisitos:**
1. Carga de datos → Spark
2. Joins → Spark
3. Agregaciones → Spark
4. Solo convierte a Pandas el resultado final (para visualización)

```python
# TODO: Implementa el análisis completo en Spark
def analisis_completo_spark():
    """
    Implementa las 4 métricas de negocio usando solo Spark:
    1. Top productos por categoría
    2. Usuarios más activos
    3. Análisis de conversión
    4. Análisis temporal
    
    Retorna: diccionario con los 4 resultados
    """
    pass

# Ejecuta y mide
start = time.time()
resultados = analisis_completo_spark()
tiempo_total = time.time() - start

print(f"Análisis completo en {tiempo_total:.2f}s")
```

**Entregable Fase 3:**
- Código completo de las 3 migraciones
- Tabla comparativa de tiempos:
  - Pandas puro
  - Híbrido (Join en Spark)
  - Spark puro
- ¿Qué enfoque es más rápido y por qué?

---

## 📊 FASE 4: Estrategia de Migración (30 minutos)

### Paso 4.1: Matriz de Decisión

Completa esta matriz para cada operación en tu código:

| Operación | Tamaño datos | Complejidad | ¿Frecuencia? | Decisión | Justificación |
|-----------|--------------|-------------|--------------|----------|---------------|
| Carga CSV | 5M → 100M | Baja | 1x/día | Spark | Volumen alto |
| Filtros simples | | | | | |
| Join 2 tablas | | | | | |
| GroupBy 3 niveles | | | | | |
| Visualización | | | | | |

**Criterios de decisión:**
- ✅ **Migrar a Spark si:**
  - Datos > 10GB
  - Joins de tablas grandes
  - GroupBy con múltiples niveles
  - Operaciones que se ejecutan frecuentemente
  
- ❌ **Mantener en Pandas si:**
  - Datos < 1GB
  - Análisis exploratorio rápido
  - Resultado final para visualización
  - Operaciones de una sola vez

---

### Paso 4.2: Plan de Migración

Crea un plan de migración para producción:

```markdown
## Plan de Migración a Spark

### Fase 1: Quick Wins (Semana 1)
- [ ] Migrar operación: ___________
- [ ] Beneficio esperado: ___________
- [ ] Riesgo: ___________

### Fase 2: Operaciones Core (Semana 2-3)
- [ ] Migrar operación: ___________
- [ ] Beneficio esperado: ___________

### Fase 3: Optimización (Semana 4)
- [ ] Tuning de particiones
- [ ] Caching estratégico
- [ ] Monitoreo de performance

### Criterios de Éxito
- Reducción de tiempo de procesamiento: ____%
- Capacidad de escalar a: _____M registros
- Costo de infraestructura: +/- ____%
```

---

## 🎓 Entregables Finales

### 1. Reporte Técnico (2-3 páginas)
Incluye:
- Resumen ejecutivo
- Análisis de cuellos de botella identificados
- Comparativa de performance (tablas y gráficas)
- Decisión de qué migrar y qué no
- Código de las migraciones implementadas

### 2. Presentación (10 minutos)
Prepara slides para presentar:
- El problema de escalabilidad
- Estrategia de migración
- Resultados obtenidos
- Lecciones aprendidas

### 3. Código Ejecutable
- Notebook con las 3 versiones (Pandas, Híbrido, Spark)
- Comentado y documentado
- Incluye mediciones de performance

---

## 📚 Recursos Adicionales

### Documentación
- [Pandas to PySpark Cheatsheet](https://spark.apache.org/docs/latest/api/python/)
- [Optimización de Spark](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

### Criterios de Evaluación
- **Identificación correcta de cuellos de botella (30%)**
- **Implementación de migraciones (40%)**
- **Justificación de decisiones (20%)**
- **Calidad del código y documentación (10%)**

---

## 💡 Tips para el Éxito

1. **No migres todo de golpe** - Empieza con las operaciones más costosas
2. **Mide siempre** - No asumas, comprueba con datos
3. **Mantén flexibilidad** - A veces un enfoque híbrido es mejor
4. **Documenta decisiones** - Explica el "por qué" de cada migración
5. **Piensa en el futuro** - ¿Tu solución escalará a 1B de registros?

---

## 🔥 Desafío Extra (Opcional)

**Escenario:** Los datos ahora llegan en streaming (100K registros/minuto)

**Tu tarea:**
- Investiga Spark Structured Streaming
- Propón cómo adaptarías tu código
- ¿Qué operaciones funcionarían en streaming?
- ¿Cuáles necesitan batch processing?

---

## ✅ Checklist de Completitud

- [ ] Fase 1: Análisis con Pandas completado
- [ ] Fase 2: Cuellos de botella identificados
- [ ] Fase 3: Al menos 2 migraciones implementadas
- [ ] Fase 4: Estrategia documentada
- [ ] Reporte técnico entregado
- [ ] Código comentado y funcional
- [ ] Presentación preparada

**¡Buena suerte! 🚀**