{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5082483e-77a9-4bcb-86f7-dc05a41cf78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Actividad: MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dab44494-cdaf-469b-9d92-68feb8bcb50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuración y carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f4e586-d0fe-4fc8-a06b-14ef99ccb947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar MLflow\n",
    "#mlflow.create_experiment(\"/Users/caog.freeman@gmail.com/wine_quality_mlops\")\n",
    "mlflow.set_experiment(\"/Users/caog.freeman@gmail.com/wine_quality_mlops\")\n",
    "\n",
    "# Cargar datos\n",
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = wine.target\n",
    "\n",
    "# Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8ec0f30-99e5-4a03-a49f-7f83676c45f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Entrenamiento sin logging de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb928fa1-3962-41fa-b5a9-b6a6a1feadfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Entrenar modelo sin tracking\n",
    "model_v1 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_v1.fit(X_train, y_train)\n",
    "\n",
    "predictions = model_v1.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "# Pregunta para reflexión: ¿Qué pasa si quiero reproducir esto mañana?\n",
    "# ¿qué información estamos perdiendo al no guardar datos del experimento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083688a8-51c4-40b0-9e11-2a60345f0461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimento con logging de MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93990fc0-5acc-4980-826a-cfd36f9041a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ahora CON tracking\n",
    "with mlflow.start_run(run_name=\"rf_baseline\") as run:\n",
    "    \n",
    "    # Parámetros\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": None,\n",
    "        \"min_samples_split\": 2,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Log de parámetros\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Entrenar\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, predictions),\n",
    "        \"f1_macro\": f1_score(y_test, predictions, average='macro')\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log del modelo\n",
    "    # IMPORTANTE: Se debe agregar un model signature como buena práctica. Pero en Databricks es obligatorio\n",
    "    # Revisa más información aquí: https://mlflow.org/docs/latest/ml/model/signatures/#how-to-log-models-with-signatures$0\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=\"wine_classifier\",\n",
    "        input_example=X_train.iloc[:5],\n",
    "    )\n",
    "    \n",
    "    # Matriz de confusión como artifact\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbb03477-4f53-4f56-9a92-4cadbd855437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Experimentando con hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e0fbb8-0f67-47eb-897d-21f6b4f367e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experimento con diferentes configuraciones\n",
    "hyperparameter_configs = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 5},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10},\n",
    "    {\"n_estimators\": 200, \"max_depth\": None},\n",
    "]\n",
    "\n",
    "best_run_id = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for i, params in enumerate(hyperparameter_configs):\n",
    "    with mlflow.start_run(run_name=f\"rf_config_{i+1}\"):\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"config_number\", i+1)\n",
    "        \n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=X_train.iloc[:5])\n",
    "        \n",
    "        # Tracking del mejor modelo\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        print(f\"Config {i+1} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Run ID: {best_run_id} with Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b183e4ac-7086-48dd-a459-a1e63d98a3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next steps\n",
    "1. Explorar la interfaz de MLflow\n",
    "2. Explorar la interfaz del model registry\n",
    "3. Desplegar nuestro modelo en un endpoint\n",
    "4. Explorar la interfaz de jobs para desplegar nuestro modelo como un Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81496910-214c-489a-b4e1-6982e166f841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Usando el endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3298c8-8838-4ddf-a0b9-9d9afac9c845",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "# Recuerden que la URL y el token son secrets. NUNCA los pongan como texto plano en producción.\n",
    "# Usen variables de entorno en su lugar\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = ''\n",
    "    headers = {'Authorization': f'Bearer ', 'Content-Type': 'application/json'}\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb06828-a660-40a6-8d63-f92ce283e02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = score_model(X_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c633fa-93b9-48f8-94e6-00eb2b7c5b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1_activity_mlops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
